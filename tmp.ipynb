{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13c88838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import udf, isnan, when, count, col, date_sub,next_day, mean, year, month, date_format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8381dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_application_name = \"Spark_Project\"\n",
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2444d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_path = 'stocks_data/MICROSOFT.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da5ac98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, schema, header=True, delimiter=';'):\n",
    "    \"\"\"Lire les données à partir d'un path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        Le chemin du fichier\n",
    "    schema : StructType\n",
    "        Le schema appliqué sur le dataframe\n",
    "    header: bool\n",
    "        Le boolean qui indique l'utilisation de la première ligne comme noms de colonnes. True par défaut\n",
    "    delimiter: \n",
    "        Définit un séparateur pour chaque champ et chaque valeur. ; par défaut\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Le DataFrame du fichier\n",
    "    \"\"\"\n",
    "    \n",
    "    extension = path.split('.')[-1]\n",
    "    if (extension == \"csv\"):\n",
    "        df = spark.read.csv(path, schema, header=header, sep=delimiter)\n",
    "    elif (extension == 'json'):\n",
    "        df = spark.read.json(path, schema)\n",
    "    else:\n",
    "        # join all csv in a folder\n",
    "        df = spark.read.csv(path, schema, header=header, sep=delimiter)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c63b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+------------+\n",
      "|               Date|              High|               Low|              Open|             Close|   Volume|         Adj Close|company_name|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+------------+\n",
      "|2017-01-03 00:00:00| 62.84000015258789|62.130001068115234|62.790000915527344| 62.58000183105469|2.06941E7| 58.67324447631836|   MICROSOFT|\n",
      "|2017-01-04 00:00:00|             62.75|62.119998931884766| 62.47999954223633| 62.29999923706055|  2.134E7| 58.41072463989258|   MICROSOFT|\n",
      "|2017-01-05 00:00:00| 62.65999984741211|62.029998779296875|62.189998626708984| 62.29999923706055| 2.4876E7| 58.41072463989258|   MICROSOFT|\n",
      "|2017-01-06 00:00:00|63.150001525878906|62.040000915527344| 62.29999923706055| 62.84000015258789|1.99229E7|58.917015075683594|   MICROSOFT|\n",
      "|2017-01-09 00:00:00| 63.08000183105469|62.540000915527344|  62.7599983215332| 62.63999938964844|2.03827E7|58.729496002197266|   MICROSOFT|\n",
      "|2017-01-10 00:00:00| 63.06999969482422|62.279998779296875| 62.72999954223633|62.619998931884766| 1.8593E7| 58.71074676513672|   MICROSOFT|\n",
      "|2017-01-11 00:00:00| 63.22999954223633| 62.43000030517578| 62.61000061035156|63.189998626708984|2.15173E7| 59.24515914916992|   MICROSOFT|\n",
      "|2017-01-12 00:00:00|63.400001525878906| 61.95000076293945|63.060001373291016| 62.61000061035156|2.09682E7| 58.70137023925781|   MICROSOFT|\n",
      "|2017-01-13 00:00:00|62.869998931884766|62.349998474121094|62.619998931884766| 62.70000076293945|1.94223E7|  58.7857551574707|   MICROSOFT|\n",
      "|2017-01-17 00:00:00| 62.70000076293945|62.029998779296875| 62.68000030517578|62.529998779296875| 2.0664E7| 58.62636947631836|   MICROSOFT|\n",
      "|2017-01-18 00:00:00| 62.70000076293945|62.119998931884766| 62.66999816894531|              62.5|1.96701E7| 58.59824752807617|   MICROSOFT|\n",
      "|2017-01-19 00:00:00| 62.97999954223633| 62.20000076293945|  62.2400016784668| 62.29999923706055|1.84517E7| 58.41072463989258|   MICROSOFT|\n",
      "|2017-01-20 00:00:00| 62.81999969482422|62.369998931884766| 62.66999816894531|  62.7400016784668|3.02135E7| 58.82326126098633|   MICROSOFT|\n",
      "|2017-01-23 00:00:00|63.119998931884766| 62.56999969482422| 62.70000076293945|62.959999084472656|2.30976E7| 59.02952575683594|   MICROSOFT|\n",
      "|2017-01-24 00:00:00|  63.7400016784668|62.939998626708984| 63.20000076293945| 63.52000045776367|2.46729E7|59.554569244384766|   MICROSOFT|\n",
      "|2017-01-25 00:00:00|  64.0999984741211| 63.45000076293945| 63.95000076293945| 63.68000030517578|2.36727E7| 59.70457458496094|   MICROSOFT|\n",
      "|2017-01-26 00:00:00| 64.54000091552734| 63.54999923706055| 64.12000274658203|  64.2699966430664|4.35546E7| 60.25773620605469|   MICROSOFT|\n",
      "|2017-01-27 00:00:00| 65.91000366210938| 64.88999938964844| 65.38999938964844| 65.77999877929688| 4.4818E7| 61.67348098754883|   MICROSOFT|\n",
      "|2017-01-30 00:00:00| 65.79000091552734| 64.80000305175781| 65.69000244140625| 65.12999725341797|3.16514E7| 61.06404495239258|   MICROSOFT|\n",
      "|2017-01-31 00:00:00|  65.1500015258789| 64.26000213623047| 64.86000061035156|  64.6500015258789|2.52705E7| 60.61402130126953|   MICROSOFT|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema\n",
    "stocksColumns = [StructField(\"Date\",TimestampType()), StructField(\"High\",DoubleType()), \n",
    "              StructField(\"Low\",DoubleType()), StructField(\"Open\",DoubleType()),\n",
    "              StructField(\"Close\",DoubleType()), StructField(\"Volume\", DoubleType()), \n",
    "              StructField(\"Adj Close\",DoubleType()), StructField(\"company_name\", StringType())]\n",
    "\n",
    "stocksSchema = StructType(stocksColumns)\n",
    "\n",
    "\n",
    "df = get_data(stocks_path, stocksSchema, delimiter=',')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ffd2d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_avg(df, period='week'):\n",
    "    \"\"\"Avoir la moyenne d'une period\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    period : string\n",
    "        la period pour la moyenne, par défaut week\n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.select('Date', 'Open', 'Close')\n",
    "    if (period == 'week'):\n",
    "        df = df.withColumn(\"week_strt_day\",date_sub(next_day(col(\"Date\"),\"sunday\"),7)) \\\n",
    "               .groupBy(\"week_strt_day\") \\\n",
    "                .agg(mean(\"Open\").alias(\"weekly_avg_open\"), mean(\"Close\").alias(\"weekly_avg_open\")) \\\n",
    "               .orderBy(\"week_strt_day\")\n",
    "    elif (period == 'month'):\n",
    "        df = df.withColumn(\"month\", date_format(col('Date'), 'yyyy/MM/1')) \\\n",
    "               .groupBy(\"month\") \\\n",
    "                .agg(mean(\"Open\").alias(\"monthly_avg_open\"), mean(\"Close\").alias('monthly_avg_close'))\\\n",
    "               .orderBy('month')\n",
    "    elif (period == 'year'):\n",
    "        df = df.withColumn(\"year\", year(col('Date'))) \\\n",
    "               .groupBy(\"year\") \\\n",
    "                .agg(mean(\"Open\").alias(\"annualy_avg_open\"), mean(\"Close\").alias('annualy_avg_close'))\\\n",
    "               .orderBy('year')\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect period format, expected week, month or year\")\n",
    "        \n",
    "    # Maybe plot a graph ?\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0c11230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+\n",
      "|    month|  monthly_avg_open| monthly_avg_close|\n",
      "+---------+------------------+------------------+\n",
      "|2017/01/1|63.185500144958496| 63.19199962615967|\n",
      "|2017/02/1| 64.13473711515728| 64.11368440326892|\n",
      "|2017/03/1| 64.76434906669284| 64.84130494490914|\n",
      "|2017/04/1| 66.23894781815379| 66.17157946134868|\n",
      "|2017/05/1| 68.82818222045898| 68.91727308793502|\n",
      "|2017/06/1| 70.56181820956144|  70.5181815407493|\n",
      "|2017/07/1| 71.84349975585937| 72.01050033569337|\n",
      "|2017/08/1|  72.7156518023947| 72.81695755668308|\n",
      "|2017/09/1|  74.3654998779297| 74.34450073242188|\n",
      "|2017/10/1| 77.89318119395863| 77.93954571810636|\n",
      "|2017/11/1| 83.64523824055989| 83.71761903308686|\n",
      "|2017/12/1| 84.83599967956543|   84.758500289917|\n",
      "|2018/01/1| 89.96666681198846| 90.07523781912667|\n",
      "|2018/02/1| 91.43736829255757| 91.36789462440892|\n",
      "|2018/03/1| 93.23047601609002| 92.89904748825799|\n",
      "|2018/04/1| 93.53095318022228| 93.21476164318267|\n",
      "|2018/05/1| 96.63272718949752| 96.98136381669478|\n",
      "|2018/06/1|100.66571517217727|100.56190454392205|\n",
      "|2018/07/1|104.58571479434059| 104.6385730561756|\n",
      "|2018/08/1|108.45391281791355|108.68434740149456|\n",
      "+---------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pd = get_period_avg(df, 'month')\n",
    "df_pd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1628e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evolution(df, period='day'):\n",
    "    \"\"\"Avoir l'évolution du prix \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    \n",
    "    period : string\n",
    "        l'échelle de l'évolution, par défaut week\n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    w = Window.partitionBy().orderBy(\"Date\")\n",
    "    if (period == 'day'): \n",
    "        df_prev = df.withColumn('prev_high', functions.lag(df['High']).over(w)) \\\n",
    "                    .withColumn('prev_low', functions.lag(df['Low']).over(w)) \\\n",
    "                    .withColumn('prev_open', functions.lag(df['Open']).over(w)) \\\n",
    "                    .withColumn('prev_close', functions.lag(df['Close']).over(w))\n",
    "    \n",
    "    elif (period == 'month'):\n",
    "        df_group_by_month = df.select('Date', 'High', 'Low', 'Open', 'Close') \\\n",
    "                              .withColumn(\"Date\", date_format(col('Date'), 'yyyy/MM/1')) \\\n",
    "                              .groupBy(\"Date\")\\\n",
    "                              .agg(functions.sum(\"High\").alias(\"High\"), \n",
    "                                   functions.sum(\"Low\").alias(\"Low\"), \n",
    "                                   functions.sum(\"Open\").alias(\"Open\"), \n",
    "                                   functions.sum(\"Close\").alias(\"Close\"))\\\n",
    "                              .orderBy('Date')\n",
    "        \n",
    "        df_prev = df_group_by_month.withColumn('prev_high', functions.lag(df_group_by_month['High']).over(w)) \\\n",
    "                    .withColumn('prev_low', functions.lag(df_group_by_month['Low']).over(w)) \\\n",
    "                    .withColumn('prev_open', functions.lag(df_group_by_month['Open']).over(w)) \\\n",
    "                    .withColumn('prev_close', functions.lag(df_group_by_month['Close']).over(w))\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect period format, expected day, month\")\n",
    "        \n",
    "    result_df = df_prev.withColumn('delta_high', df_prev['High'] - df_prev['prev_high']) \\\n",
    "                   .withColumn('delta_low', df_prev['Low'] - df_prev['prev_low']) \\\n",
    "                   .withColumn('delta_open', df_prev['Open'] - df_prev['prev_open']) \\\n",
    "                   .withColumn('delta_close', df_prev['Close'] - df_prev['prev_close']) \\\n",
    "                   .select('Date', 'delta_high', 'delta_low', 'delta_open', 'delta_close')\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "958e388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 11:15:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/18 11:15:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/18 11:15:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/18 11:15:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/18 11:15:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-------------------+-------------------+\n",
      "|     Date|         delta_high|          delta_low|         delta_open|        delta_close|\n",
      "+---------+-------------------+-------------------+-------------------+-------------------+\n",
      "|2017/01/1|               null|               null|               null|               null|\n",
      "|2017/02/1| -47.19999694824219| -43.17999267578125| -45.14999771118164|-45.679988861083984|\n",
      "|2017/03/1|  275.0299873352051|  270.3399963378906| 271.02002334594727|  273.1900100708008|\n",
      "|2017/04/1|-234.58000946044922|-231.86999130249023|-231.04001998901367|-234.09000396728516|\n",
      "|2017/05/1|   258.760009765625| 255.37997436523438| 255.68000030517578|  258.9199981689453|\n",
      "|2017/06/1| 39.709991455078125| 30.659996032714844| 38.139991760253906|  35.21998596191406|\n",
      "|2017/07/1|-114.06000518798828|-107.55998992919922|-115.49000549316406|-111.18998718261719|\n",
      "|2017/08/1| 235.25000762939453|  233.7400131225586| 235.58999633789062| 234.58001708984375|\n",
      "|2017/09/1| -187.7899932861328|-184.74002075195312|-185.14999389648438|-187.90000915527344|\n",
      "|2017/10/1|  227.9699935913086| 227.84002685546875|  226.3399887084961| 227.77999114990234|\n",
      "|2017/11/1| 42.159996032714844|  40.66999816894531|  42.90001678466797| 43.399993896484375|\n",
      "|2017/12/1| -57.65998077392578| -63.06000518798828| -59.83000946044922|-62.899993896484375|\n",
      "|2018/01/1| 195.62000274658203| 193.54000854492188| 192.58000946044922|  196.4099884033203|\n",
      "|2018/02/1|-141.26002502441406| -165.8300018310547|-151.99000549316406|-155.58999633789062|\n",
      "|2018/03/1| 215.92001342773438| 218.62000274658203| 220.52999877929688| 214.88999938964844|\n",
      "|2018/04/1| 5.4400177001953125| 3.7099990844726562|  6.310020446777344|  6.629997253417969|\n",
      "|2018/05/1|  161.8699951171875| 178.18999481201172| 161.76998138427734| 176.08000946044922|\n",
      "|2018/06/1|-17.790023803710938|-14.490013122558594|-11.939979553222656|-21.790008544921875|\n",
      "|2018/07/1|  86.58000183105469|  79.95999908447266|  82.31999206542969|  85.61003875732422|\n",
      "|2018/08/1| 297.11000061035156|  303.5999984741211|  298.1399841308594|  302.3299560546875|\n",
      "+---------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_evol = get_evolution(df, 'month')\n",
    "df_evol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a45f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_with(df1, df2):\n",
    "    df2_pd = df2.toPandas()\n",
    "    df1_pd = df1.toPandas()\n",
    "     \n",
    "    df1_name = df1_pd.iloc[0, -1]\n",
    "    df2_name = df2_pd.iloc[0, -1]\n",
    "    \n",
    "    df1_pd = df1_pd.drop(columns=['Date', 'company_name'])\n",
    "    df2_pd = df2_pd.drop(columns=['Date', 'company_name'])\n",
    "    \n",
    "    cols_1 = df1_pd.columns\n",
    "    cols_2 = df2_pd.columns\n",
    "    \n",
    "    df1_pd[cols_1] = df1_pd[cols_1].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "    df2_pd[cols_2] = df2_pd[cols_2].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "    new_cols_1 = [col + ' ' + df1_name for col in cols_1]\n",
    "    new_cols_2 = [col + ' ' + df2_name for col in cols_2]\n",
    "    df1_pd.columns = new_cols_1\n",
    "    df2_pd.columns = new_cols_2\n",
    "    \n",
    "    corr = pd.concat([df1_pd,df2_pd],axis=1).corr()\n",
    "    corr = corr.loc[new_cols_1, new_cols_2]\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111)\n",
    "    ax.set_title(\"Correlation Matrix\")\n",
    "    ax.set_xticklabels(['']+new_cols_1, rotation=90, ha='left')\n",
    "    ax.set_yticklabels(['']+new_cols_2)\n",
    "    cax=ax.matshow(corr,vmax=1,vmin=-1)\n",
    "    fig.colorbar(cax)\n",
    "    #plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
